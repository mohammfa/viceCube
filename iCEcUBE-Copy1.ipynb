{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammfa/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all values greater than a certain value\n",
    "def findGreater(mat,val):\n",
    "    arr = np.array([])\n",
    "    for i in range(mat.shape[0]): #xaxis\n",
    "        for j in range(mat.shape[1]): #yaxix\n",
    "            if(mat[i,j] > val):\n",
    "                arr = np.append(arr,mat[i,j])\n",
    "                \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Def Generate neutrino \n",
    "Random origin (x,y)\n",
    "Random direction (dx, dy) or (one angles)\n",
    "Random energy (0 to 1) or (10 to 300 nanoseconds)\n",
    "Random time\n",
    "(make code adjustable)\n",
    "We want to know how fast the neutrino is\"\"\"\n",
    "def generateNeutrino():\n",
    "    x = random.uniform(0,10)\n",
    "    y = random.uniform(0,10)\n",
    "    angle = random.vonmisesvariate(0, 0)\n",
    "    energy = random.random();\n",
    "    time = random.randint(10,300)\n",
    "\n",
    "    return x, y, angle, energy, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSignal(x,y,angle,energy,time):\n",
    "    nodes = 10\n",
    "    x_arr = np.linspace(0,nodes,nodes + 1)\n",
    "    y_arr = np.linspace(0,nodes,nodes + 1)\n",
    "    #10 by 10 grid which is the VICE CUBE\n",
    "    \n",
    "    #using MeshGrid\n",
    "    mesh_x, mesh_y = np.meshgrid(x_arr, y_arr)\n",
    "\n",
    "    #find distance of each sensor to the origion using euclidean distance formula\n",
    "    x_sq = mesh_x - x\n",
    "    y_sq = mesh_y - y\n",
    "    x_sq = np.power(x_sq,2)\n",
    "    y_sq = np.power(y_sq,2)\n",
    "    distance = np.sqrt(x_sq + y_sq)\n",
    "    \n",
    "    #find charge\n",
    "    #charge formula: C = (maxDistance - distance_to_each_node)^2 / maxDistance\n",
    "    maxDistance = math.sqrt((nodes * nodes) + (nodes * nodes))\n",
    "    charge = np.power((maxDistance - distance),2) / pow(maxDistance,2)\n",
    "    \n",
    "    \n",
    "    #based on the angle and the origin create a line (data fit) that would simulate a neutrino path\n",
    "    return mesh_x, mesh_y, distance, charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exampleRun():\n",
    "    x,y,angle,energy,time = generateNeutrino();\n",
    "    mesh_x, mesh_y, distance, charge = generateSignal(x,y,angle,energy,time)\n",
    "    #print(\"x\\n\", mesh_x)\n",
    "    #print(\"y\\n\", mesh_y)\n",
    "\n",
    "    plt.plot(mesh_x,mesh_y, marker='.', color='k', linestyle='none')\n",
    "    \n",
    "    \n",
    "    CS = plt.contourf(mesh_x, mesh_y,charge, 15, cmap=plt.cm.rainbow,\n",
    "                  vmax=charge.max(), vmin=0)\n",
    "    plt.colorbar()  \n",
    "    plt.show()\n",
    "\n",
    "    print(\"x y angle energy time\")\n",
    "    print(x,y,angle,energy,time)\n",
    "    \n",
    "    #print(\"distance\\n\", distance)\n",
    "    print(\"charge\\n\", charge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#decompose signal function\n",
    "\"\"\"\" function to decompose the signal to create a training vector and a training label\n",
    "def decomposeSignal():\n",
    "    \"\"\"\n",
    "#plan is to use have a vector of which sensors detected the neutrino and then have a vector for distance, charge\n",
    "#then use that as the training data\n",
    "\n",
    "def createTrainingVector(dataSize):\n",
    "    #we changed from array of arrays to 2D array\n",
    "    #we changed data types\n",
    "    #also we changed feature columns shape to specify how many features we have\n",
    "    xy = np.zeros(2)\n",
    "    trainVec = np.zeros((dataSize,121),dtype = np.float32)\n",
    "    trainLabel = np.zeros(dataSize, dtype = np.float32)\n",
    "\n",
    "    for i in range(dataSize):\n",
    "        x_o,y_o,angle,energy,time = generateNeutrino();\n",
    "        mesh_x, mesh_y, distance, charge = generateSignal(x_o,y_o,angle,energy,time)\n",
    "        #xy[0] = x_o\n",
    "        #xy[1] = y_o\n",
    "        #flattening the charge matrix to a vector\n",
    "        chargeVector = charge.flatten()\n",
    "        trainVec[i,:] = chargeVector\n",
    "        trainLabel[i] = x_o\n",
    "    \n",
    "    return trainVec, trainLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputFunction(datasize=10):\n",
    "    trainVec, trainLabel = createTrainingVector(dataSize)    \n",
    "    features = {'x' : trainVec}\n",
    "    #trainLabel is the output. which currently is the origin\n",
    "    labels = trainLabel\n",
    "    #labels = np.int(labels)\n",
    "    \n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for some reason this does not work. I however found another approach with using tf.estimator.inputs.numpy_input_fn\n",
    "def trainInputFn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    print(type(features))\n",
    "\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    return dataset.shuffle(1000).repeat().batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "float32\n",
      "(120, 121)\n",
      "(120,)\n",
      "feature_columns= [_NumericColumn(key='x', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/gn/y5s233mx34b21b029nk5sc3h0000gn/T/tmphku8i792\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/gn/y5s233mx34b21b029nk5sc3h0000gn/T/tmphku8i792', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1056ec860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/gn/y5s233mx34b21b029nk5sc3h0000gn/T/tmphku8i792/model.ckpt.\n",
      "INFO:tensorflow:loss = 3260.6074, step = 1\n",
      "INFO:tensorflow:global_step/sec: 559.81\n",
      "INFO:tensorflow:loss = 25.050863, step = 101 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 924.385\n",
      "INFO:tensorflow:loss = 7.3789988, step = 201 (0.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 959.545\n",
      "INFO:tensorflow:loss = 3.5290542, step = 301 (0.104 sec)\n",
      "INFO:tensorflow:global_step/sec: 840.124\n",
      "INFO:tensorflow:loss = 2.9279563, step = 401 (0.120 sec)\n",
      "INFO:tensorflow:global_step/sec: 855.975\n",
      "INFO:tensorflow:loss = 1.7469335, step = 501 (0.116 sec)\n",
      "INFO:tensorflow:global_step/sec: 899.345\n",
      "INFO:tensorflow:loss = 2.164939, step = 601 (0.111 sec)\n",
      "INFO:tensorflow:global_step/sec: 897.519\n",
      "INFO:tensorflow:loss = 1.3418367, step = 701 (0.111 sec)\n",
      "INFO:tensorflow:global_step/sec: 891.25\n",
      "INFO:tensorflow:loss = 1.0459797, step = 801 (0.112 sec)\n",
      "INFO:tensorflow:global_step/sec: 775.819\n",
      "INFO:tensorflow:loss = 1.3385265, step = 901 (0.129 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /var/folders/gn/y5s233mx34b21b029nk5sc3h0000gn/T/tmphku8i792/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.0505812.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(train_input_fn)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if __name__== \"__main__\":\n",
    "dataSize = 120\n",
    "features, labels = inputFunction(dataSize)\n",
    "\n",
    "print((features['x'].dtype))\n",
    "print(labels.dtype)\n",
    "print(features['x'].shape)\n",
    "print(labels.shape)\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(x=(features),\n",
    "                                                    y=(labels),num_epochs=None,shuffle=True)\n",
    "\n",
    "my_feature_columns = []\n",
    "for key in features.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
    "\n",
    "print(\"feature_columns=\",my_feature_columns)\n",
    "\n",
    "# Specify that all features have real-value data\n",
    "my_feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[121])]\n",
    "\n",
    "# Build 3 layer DNN with 10, 20, 10 units respectively.\n",
    "regressor = tf.estimator.DNNRegressor(feature_columns=my_feature_columns,\n",
    "                                      hidden_units=[10, 20, 10])\n",
    "\n",
    "regressor.train(input_fn=train_input_fn, steps=1000)\n",
    "\n",
    "\"\"\"\n",
    "print(train_input_fn)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md               iCEcUBE-Copy2.ipynb     iris_training.csv\r\n",
      "helloWorldExample.ipynb iCEcUBE.ipynb\r\n",
      "iCEcUBE-Copy1.ipynb     iris_test.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
